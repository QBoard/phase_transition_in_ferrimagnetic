\section{Introduction}

Machine learning techniques as a data analysis tool has been proving its efficiency in numerous research areas during the last decade.
The reason for such popularity is the core idea behind all the employed methods â€“ inherent data pattern recognition, that is an essential part of scientific activity.
Physics is no exception since multiple ideas from statistical physics lie in the machine learning's foundation.

The variety of the machine learning methods can be distinguished into 2 groups: supervised and unsupervised methods.
First one requires categorised data as an input to compare the algorighm performance with the known label.
Unsupervised approaches need no prior data labeling and classifies input based on the hidden inner properties.

In physics classifiyng phases of matter is one of the key problems.
Plenty of job was made in this direction with help of moder data science techniques.
Previously some job was done in this area.

\section{Hamiltonian}

We are given \textcolor{red}{(how? Elaboration is required from Sergey)} the hamiltonian:

\begin{multline}
    H = -x\bm{M}_f\bm{H}_{eff}-(1-x)\bm{M}_d\bm{H}- xK_f\left(\frac{\bm{M}_f}{M_f}\bm{Z}\right)^2\\
    -(1-x)K_d\cos^2\theta_d\left(\frac{\bm{M}_d}{M_d}\bm{Z}\right)^2,
\end{multline}

where:
$\bm{H}$ - external field
$\bm{M}_f$, $\bm{M}_d$ - magnetization
$\bm{H}_{eff} = \bm{H} - \lambda\bm{M}_d$
$\bm{Z}$ - anisotropy vector.

Let's introduce $\theta_d$ as an angle between $\bm{M}_d$ and $Z$. In zero approximation $\bm{Z} || \bm{H}$ so we obtain:
\begin{multline}
    H = -xM_fH_{eff}-(1-x)M_dH\cos\theta_d- xK_f\left(\frac{\bm{M}_f}{M_f}\bm{Z}\right)^2\\
    -(1-x)K_d\cos^2\theta_d
\end{multline}

From physical observation we know that $\bm{M}_f = \chi_f \bm{H}_{eff}$, so:
\begin{equation}
    \frac{\bm{M}_f}{M_f}\bm{Z} =
    \frac{\bm{H}_{eff}}{H_{eff}}\bm{Z} =
    \frac{\bm{H} - \lambda\bm{M}_d}{H_{eff}}\bm{Z}=
    \frac{H - \lambda M_d \cos\theta_d}{H_{eff}}
\end{equation}
Defining the last expression as $\cos\theta_f$ we get:

\begin{multline}
    H = -xM_fH_{eff} -
    (1-x)M_dH\cos\theta_d -
    xK_f\cos^2\theta_f -\\
    (1-x)K_d\cos^2\theta_d
\end{multline}
With constants for the further computations set to:
    $\mu_{B} = 9.27\cdot10^{-21} \mathrm{erg}\cdot \mathrm{G}^{-1}$
    $M_{f}=10 \mu_{B}$,
    $M_{d}=5 \mu_{B}$,
    $K_{d,f} = 2.55 \cdot 10^{-16}$,
    $\lambda = 10^5 \cdot \mu_{B}^{-1}$


\section{Learning by confusion}
For phase transition identifying we use the method called "learning by confusion" originally proposed in \cite{VanNieuwenburg2017}.
It uses the data set that depends on some parameter $c$ that lies in the range $[c_0, c_1]$ and depict system properties.
Assuming that information about the current phase is encoded in this data we can teach the algorithm to categorise data points into 2 groups.
These two groups would correspond to the system's phases.

However, it is unclear how to train the algorithm since we have no prior information about the proper phase attribution.
That is where learning by confusion enters to the scene.
We choose parameter $c'\in[c_0, c_1]$, and set label $0$ to all the data with parameter value less then $c'$ and label $1$ to the rest.
By doing so we guess critical parameter $c'$ and train the algorithm to learn hidden patterns in the data.
If our guess is correct, the algorithm would learn the hidden properties and predict phase attribution with high accuracy.
Otherwise, algorithm would be confused, would not learn any patterns and fail during the prediction phase.
Applying this procedure on the grid of $c'\in[c_0, c_1]$ we would obtain the $W$-shape where central peak would depict the critical point.\footnote{If system has several phase transition points, number of peaks increase accordingly}

In the case of GdFeCo, we have the data that depends on 2 parameters: $H$ and $x$.
For each set of parameters there is an angle $\theta_d$ at which energy minimum is reached.
This angle experiences a leap that indicates a phase transition to identify.
So we have 2 problems: 1) learning by confusion is originally designed only for 1 parameter; 1) there is no information about the $\theta_d$ that provides the minimum;

The former problem is easily solved by the following approach.
We fix one parameter, say $H$, run the algorithm and obtain the $W$-shape.
Then we perform this step for the whole grid of $H$ and get a 2D surface where ridges show the phase transition lines.

Monte-Carlo inspired approach was applied to tackle the latter problem.
We sample $n_{\theta_d}$ angles $\theta_d \sim \mathcal{U}(0, \pi)$ for each set of parameters ($H$, $x$).
Then calculate hamiltonian values in each dot and find the angle $\hat{\theta_d}$ when energy is the smallest.
This estimation is our input vector for the following algorithm operations.

For model learning an \texttt{XGBoost} algorithm from \texttt{sklearn} library was used.
It is a powerful realisation of decision trees building algorithm.
Labeled data is split into 2 parts: \texttt{train} and \texttt{test}.
The first is used for model training while the second is for accuracy computation.

For noise elimination models (and corresponding w-shapes) was built $n_{sampl}$ times and then accuracy values were averaged.


\section{Theoretical predictions}
\textcolor{red}{Sergey part?}

\section{Results}

